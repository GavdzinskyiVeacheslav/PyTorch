import torch

x = torch.randn([1, 100, 3])
target = torch.randn([1, 100, 64])


class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fct = torch.nn.Linear(3, 64) # wx + b where w - is weight and b - is bias

    def forward(self, x): # __call__, which call method forward
        x = self.fct(x)
        return x


model = Net()
print(model(x))
# tensor([[[ 0.6115,  0.7948,  0.6673,  ...,  1.2753,  0.3922,  0.5839],
#          [ 0.4101,  0.2289, -0.1507,  ...,  0.4994, -0.2513, -0.2651],
#          [ 0.9404,  0.8389,  0.4811,  ...,  0.5663, -0.2476, -0.3147],
#          ...,
#          [ 1.3069,  1.1446,  0.3490,  ..., -0.9395, -1.3535, -1.5059],
#          [-0.5779, -0.4584, -0.9580,  ..., -0.0425, -0.3733,  0.0486],
#          [ 1.1822,  0.8340,  0.4103,  ...,  0.4190, -0.4757, -0.7690]]],
#        grad_fn=<AddBackward0>)
#
# Process finished with exit code 0
print(model(x).shape)
# torch.Size([1, 100, 64])
criterion = torch.nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.0003)
for epoch in range(10):
    output = model(x)
    loss = criterion(output, target)
    print("Epoch: ", epoch, 'Loss: ', loss.item())
    optimizer.zero_grad()  # w.grad and b.grad - reset to zero
    loss.backward() # updates w.grad and b.grad, using private detectives
    optimizer.step() #  w += -lr * w.grad

# Epoch:  0 Loss:  1.3829896450042725
# Epoch:  1 Loss:  1.3829807043075562
# Epoch:  2 Loss:  1.3829717636108398
# Epoch:  3 Loss:  1.382962942123413
# Epoch:  4 Loss:  1.3829540014266968
# Epoch:  5 Loss:  1.3829450607299805
# Epoch:  6 Loss:  1.3829361200332642
# Epoch:  7 Loss:  1.3829272985458374
# Epoch:  8 Loss:  1.382918357849121
# Epoch:  9 Loss:  1.3829094171524048

print(model.parameters())
# <generator object Module.parameters at 0x000001B2FE958258>

print(list(model.parameters()))
# [Parameter containing:
# tensor([[-0.4585, -0.4711,  0.3551],
#         [ 0.4177,  0.0718,  0.1333],
#         [ 0.0092, -0.0161,  0.0810],
#         [-0.2134, -0.4312, -0.5144],
#         [ 0.5498,  0.4333,  0.3140],
#         [ 0.4304, -0.1793, -0.2847],
#         [-0.1752, -0.0803,  0.2720],
#         [-0.1373,  0.4307,  0.4965],
#         [ 0.4778,  0.1973, -0.0015],
#         [-0.0671, -0.5440, -0.5467],
#         [ 0.4024,  0.3432,  0.4721],
#         [ 0.1239,  0.2213,  0.3189],
#         [-0.2914, -0.4255, -0.5106],
#         [ 0.5615, -0.0830,  0.1295],
#         [-0.2025,  0.1528,  0.4876],
#         [-0.2387, -0.3730, -0.1750],
#         [ 0.0745, -0.3061, -0.4182],
#         [ 0.0420, -0.2185, -0.1398],
#         [-0.2394,  0.0409, -0.1598],
#         [ 0.2571, -0.2650, -0.0910],
#         [ 0.5511, -0.1938,  0.1499],
#         [-0.2459,  0.1809, -0.3803],
#         [ 0.4752,  0.1540, -0.2685],
#         [-0.5640,  0.2814,  0.0240],
#         [ 0.0635, -0.1565,  0.1694],
#         [-0.3108,  0.2851, -0.1319],
#         [ 0.3434,  0.5630,  0.5446],
#         [-0.1122, -0.2681, -0.3605],
#         [ 0.0972,  0.3066, -0.2911],
#         [-0.3814,  0.3329,  0.5229],
#         [ 0.0629, -0.3161,  0.1118],
#         [ 0.2515,  0.0206,  0.4218],
#         [ 0.0010,  0.4994, -0.0537],
#         [ 0.2377,  0.5385, -0.0785],
#         [-0.0529, -0.3955, -0.0777],
#         [ 0.3305,  0.2834,  0.2884],
#         [ 0.1671,  0.1211, -0.2219],
#         [-0.2635, -0.4592, -0.2951],
#         [ 0.4013, -0.4803,  0.3961],
#         [ 0.1056,  0.4094, -0.0273],
#         [ 0.2904, -0.0435,  0.2745],
#         [ 0.0364, -0.0257, -0.0688],
#         [ 0.4693,  0.2272,  0.5087],
#         [-0.4034,  0.4209,  0.1414],
#         [-0.0083, -0.4546, -0.2755],
#         [-0.4200,  0.2484,  0.3833],
#         [ 0.3747,  0.4383,  0.2091],
#         [ 0.1201, -0.4337,  0.0477],
#         [-0.2455,  0.3822, -0.0828],
#         [-0.4836,  0.3024,  0.3832],
#         [ 0.5244,  0.3561, -0.3541],
#         [ 0.2820,  0.5648,  0.5727],
#         [-0.0020, -0.1530,  0.2607],
#         [ 0.3870,  0.2172,  0.0981],
#         [-0.2723, -0.1833, -0.1247],
#         [ 0.0176, -0.2602,  0.0538],
#         [-0.0244, -0.1459,  0.3706],
#         [-0.5615,  0.2020,  0.2446],
#         [-0.4929, -0.4386, -0.4845],
#         [ 0.4355, -0.5474, -0.1019],
#         [ 0.3933,  0.2622,  0.4049],
#         [-0.3133,  0.5272, -0.5754],
#         [-0.4866,  0.3579, -0.2944],
#         [ 0.2733,  0.1665,  0.2554]], requires_grad=True), Parameter containing:
# tensor([-0.5754,  0.4106,  0.0854, -0.1077,  0.1594,  0.0897,  0.3936, -0.5764,
#         -0.1369, -0.0078, -0.3242, -0.4432, -0.5015,  0.5380,  0.4231, -0.5058,
#         -0.2234, -0.4024, -0.4598,  0.4353, -0.1236, -0.0508,  0.4751,  0.3544,
#          0.1751,  0.2742, -0.2637,  0.3897,  0.4809, -0.2953, -0.1082,  0.1661,
#          0.2713,  0.1794, -0.0526, -0.0676, -0.5024, -0.4644,  0.3544,  0.2314,
#         -0.1447,  0.1763,  0.2621,  0.3746,  0.2887,  0.3942, -0.0741,  0.4503,
#         -0.4329,  0.4416, -0.5589,  0.3280,  0.5727, -0.1105, -0.4041, -0.5258,
#         -0.5092,  0.4696, -0.2160,  0.0411, -0.1242, -0.1949, -0.2441,  0.3851],
#        requires_grad=True)]
#
# Process finished with exit code 0








